{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import numpy as np\n",
    "from pyspark.mllib.stat import Statistics\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. given a target column and a dataset as input, compute the correlation between that \n",
    "### target column and all the features in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "df = spark.read.csv('/Users/xue/Desktop/Farrago/Datasets/house price demo/kc_house_data.csv',\n",
    "                     header=True,\n",
    "                     inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation(df, target_col):\n",
    "    # drop string columns\n",
    "    columns_to_drop = [item[0] for item in df.dtypes if item[1].startswith('string')]\n",
    "    df_numeric = df.drop(*columns_to_drop)\n",
    "    \n",
    "    # generate correlation matrix\n",
    "    features = df_numeric.rdd.map(lambda row: row[0:])\n",
    "    corr_mat=Statistics.corr(features, method=\"pearson\")\n",
    "    corr_df = pd.DataFrame(corr_mat)\n",
    "    corr_df.index, corr_df.columns = df_numeric.columns, df_numeric.columns\n",
    "    corr_df = corr_df[target_col]\n",
    "    \n",
    "    return corr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id              -0.016762\n",
      "price            1.000000\n",
      "bedrooms         0.308350\n",
      "bathrooms        0.525138\n",
      "sqft_living      0.702035\n",
      "sqft_lot         0.089661\n",
      "floors           0.256794\n",
      "waterfront       0.266369\n",
      "view             0.397293\n",
      "condition        0.036362\n",
      "grade            0.667434\n",
      "sqft_above       0.605567\n",
      "sqft_basement    0.323816\n",
      "yr_built         0.054012\n",
      "yr_renovated     0.126434\n",
      "zipcode         -0.053203\n",
      "lat              0.307003\n",
      "long             0.021626\n",
      "sqft_living15    0.585379\n",
      "sqft_lot15       0.082447\n",
      "Name: price, dtype: float64\n",
      "3.3591129779815674\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(correlation(df, 'price'))\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 2 \n",
    "#### This will be a little more challenging. At the time we implement an outlier detection module in order to remove outliers from our data.\n",
    "#### We never implemented that for Big Data, and it’s now time to do that with Spark.\n",
    "#### Before implementing anything, let’s do a little bit of research. This is a good starting point:\n",
    "#### https://towardsdatascience.com/a-brief-overview-of-outlier-detection-techniques-1e0b2c19e561\n",
    "\n",
    "#### Once you find a nice outlier detection technique, just tell me and we decide which one to implement. If feasible, we can also decide to implement more than one in order to make our future framework richer. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 3\n",
    "### rotate an image\n",
    "#### This is more of an experiment. Spark is apparently not well developed for dealing with large datasets of images. I just want you to \n",
    "\n",
    "#### apply a simple rotation of 90 degrees to an image by using this module, if possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = spark.read.format(\"image\").load(\"path/to/images/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
